import json
import logging
import httpx
import backend.utils as utils
import backend.config as config
import ollama

from functools import lru_cache
from typing import Annotated, AsyncGenerator, Dict, List
from typing import Optional
from fastapi import Depends, FastAPI, HTTPException, Header, Request
from fastapi.responses import FileResponse, RedirectResponse, StreamingResponse
from pydantic import BaseModel, Field

from backend.dependencies import get_llama_streamer

from google import genai
from google.genai.errors import APIError
from openai import OpenAI, APIError
from anthropic import Anthropic, APIError as AnthropicAPIError

from fastapi.middleware.cors import CORSMiddleware
from backend.constants import SYSTEM_PROMPT, SYSTEM_PROMPT_FOR_SNIPPETS

@lru_cache
def get_settings():
    return config.Settings()

try:
    settings = get_settings()
    client = ollama.AsyncClient(host=settings.OLLAMA_HOST)
except Exception as e:
    logging.error(f"Failed to initialize Ollama client: {e}")
    client = None

app = FastAPI(
    title="Ollama Code Analysis API",
    description="An API endpoint to analyze code snippets using the Ollama LLM.",
    version="1.0.0",
)

origins = [
    "http://localhost:5500",
    "http://127.0.0.1:5500",
    "http://localhost:3000",
    "http://127.0.0.1:3000",
    "http://localhost:8000",
    "http://127.0.0.1:8000",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  
    allow_credentials=True,  
    allow_methods=["*"],  
    allow_headers=["*"],  
)


class CodeAnalysisRequest(BaseModel):
    code: str = Field(..., description="The code snippet to be analyzed.")
    context: Optional[str] = Field(
        None, description="Optional context about the code's purpose."
    )

class APIKey(BaseModel):
    key: str = Field("", description="The encrypted key.")
    model_id: str = Field("", description="The model related to the key.")
    url: Optional[str] = Field("", description="Optional URL (only for local providers)")

class APIKeyPayload(BaseModel):
    data: Dict[str, List[APIKey]]

@app.post("/analyze", tags=["Proxy Route"])
async def proxy_via_headers(request: Request):

    useLocalProvider = True if request.headers["x-use-local-provider"] == 'true' else False
    useSnippetModel = True if request.headers["x-use-snippet-model"] == 'true' else False
    defaultLocalProvider = request.headers["x-default-local-provider"]
    defaultCloudProvider = request.headers["x-default-cloud-provider"]

    if useLocalProvider and useSnippetModel:
        return RedirectResponse(f"/analyze_snippet_{defaultLocalProvider}")
    elif useLocalProvider and not useSnippetModel:
        return RedirectResponse(f"/analyze_code_{defaultLocalProvider}")
    elif not useLocalProvider and useSnippetModel:
        return RedirectResponse(f"/analyze_snippet_{defaultCloudProvider}")
    elif not useLocalProvider and not useSnippetModel:
        return RedirectResponse(f"/analyze_code_{defaultCloudProvider}")


@app.post("/analyze_code_srvllama", tags=["Analysis"])
async def analyze_code_endpoint_llama_server(request_data: CodeAnalysisRequest, settings: Annotated[config.Settings, Depends(get_settings)], x_local_alignment_model: str | None = Header(default=None)):

    if not x_local_alignment_model:
        raise HTTPException(
            status_code=400,
            detail="Invalid model name"
        )

    user_content = f"CODE SNIPPET:\n---\n{request_data.code}\n---"
    if request_data.context:
        user_content += f"\nADDITIONAL CONTEXT:\n---\n{request_data.context}\n---"

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_content},
    ]

    payload = {
        "model": x_local_alignment_model,
        "messages": messages,
        "stream": True,
        "temperature": 0.5,
    }

    async def generate_stream() -> AsyncGenerator[str, None]:
        async with httpx.AsyncClient(timeout=None) as client:
            try:
                async with client.stream(
                    "POST", settings.LLAMA_SERVER_URL, json=payload
                ) as response:

                    if response.status_code != 200:
                        error_msg = await response.aread()
                        logging.error(f"Llama Server Error: {error_msg.decode()}")
                        raise HTTPException(
                            status_code=response.status_code,
                            detail=f"Llama server returned error: {response.status_code}",
                        )

                    async for line in response.aiter_lines():
                        if line.startswith("data: "):
                            data_str = line[6:]  

                            if data_str.strip() == "[DONE]":
                                break

                            try:
                                data_json = json.loads(data_str)
                                
                                delta = data_json.get("choices", [{}])[0].get(
                                    "delta", {}
                                )
                                content = delta.get("content", "")

                                if content:
                                    yield content
                            except json.JSONDecodeError:
                                logging.warning(f"Failed to parse JSON line: {line}")
                                continue

            except httpx.ConnectError:
                yield "\n[SERVER_ERROR] Could not connect to llama-server at localhost:8080. Is it running?"
            except Exception as e:
                logging.error(f"An unexpected error occurred: {e}")
                yield f"\n[SERVER_ERROR] An unexpected error occurred: {str(e)}"

    return StreamingResponse(generate_stream(), media_type="text/plain")


@app.post("/analyze_snippet_srvllama", tags=["Analysis"])
async def analyze_snippet_llama_server_endpoint(
    request_data: CodeAnalysisRequest,
    settings: Annotated[config.Settings, Depends(get_settings)],
    llama_streamer = Depends(get_llama_streamer),
    x_local_snippet_model: str | None = Header(default=None),
):

    if not x_local_snippet_model:
        raise HTTPException(status_code=400, detail="Invalid model name")

    payload = {
        "model": x_local_snippet_model,
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT_FOR_SNIPPETS},
            {"role": "user", "content": request_data.code},
        ],
        "stream": True,
        "temperature": 0.5,
    }

    async def generate_stream() -> AsyncGenerator[str, None]:
        async for chunk in llama_streamer(settings.LLAMA_SERVER_URL, payload):
            yield chunk

    return StreamingResponse(generate_stream(), media_type="text/plain")

@app.post("/analyze_snippet_ollama", tags=["Analysis"])
async def analyze_snippet_endpoint(request_data: CodeAnalysisRequest, x_local_snippet_model: str | None = Header(default=None)):

    model = x_local_snippet_model

    if not model:
        raise HTTPException(
            status_code=400,
            detail="No model provided"
        )

    if client is None:
        raise HTTPException(
            status_code=503,
            detail="Ollama client is not initialized. Ensure Ollama is running and accessible.",
        )

    full_prompt = f"{request_data.code}"
    
    async def generate_stream() -> AsyncGenerator[str, None]:
        try:
            if client is None:
                raise HTTPException(
                    status_code=503,
                    detail="Ollama service is unavailable.",
                )
            stream = await client.generate(
                model=model, 
                prompt=full_prompt, 
                system=SYSTEM_PROMPT_FOR_SNIPPETS, 
                stream=True
            )

            async for chunk in stream:
                response_text = chunk.get("response", "")
                if response_text:
                    
                    
                    yield response_text
                    
        except Exception as e:
            logging.error(f"An unexpected error occurred: {e}")
            yield f"\n[SERVER_ERROR] An unexpected error occurred: {e}"

    return StreamingResponse(
        generate_stream(), media_type="text/plain" 
    )


@app.post("/analyze_code_ollama", tags=["Analysis"])
async def analyze_code_endpoint(request_data: CodeAnalysisRequest, x_local_alignment_model: str | None = Header(default=None)):

    if not x_local_alignment_model:
        raise HTTPException(
            status_code=400,
            detail="Invalid model name"
        )


    if client is None:
        raise HTTPException(
            status_code=503,
            detail="Ollama client is not initialized. Ensure Ollama is running and accessible.",
        )

    full_prompt = f"{request_data.code}"
    
    async def generate_stream() -> AsyncGenerator[str, None]:
        try:
            if client is None:
                raise HTTPException(
                    status_code=503,
                    detail="Ollama service is unavailable.",
                )
            
            stream = await client.generate(
                model=x_local_alignment_model, 
                prompt=full_prompt, 
                system=SYSTEM_PROMPT, 
                stream=True
            )

            async for chunk in stream:
                response_text = chunk.get("response", "")
                if response_text:
                    
                    
                    yield response_text
                    
        except Exception as e:
            logging.error(f"An unexpected error occurred: {e}")
            yield f"\n[SERVER_ERROR] An unexpected error occurred: {e}"

    return StreamingResponse(
        generate_stream(), media_type="text/plain" 
    )


@app.post("/analyze_snippet_gemini", tags=["Analysis"])
@app.post("/analyze_code_gemini", tags=["Analysis"])
async def analyze_code_endpoint_gemini(request_data: CodeAnalysisRequest, settings: Annotated[config.Settings, Depends(get_settings)] , 
                                       x_use_snippet_model: str | None = Header(default=None), 
                                       x_cloud_api_key: str | None = Header(default=None), 
                                       x_cloud_encrypted_key: str | None = Header(default=None), 
                                       x_cloud_iv: str | None = Header(default=None)):

    api_key = ""
    if x_cloud_api_key and x_cloud_encrypted_key and x_cloud_iv:
        api_key = utils.decrypt_envelope(x_cloud_encrypted_key, x_cloud_iv, x_cloud_api_key, settings.RSA_PRIVATE_KEY)

    try:
        gclient = genai.Client(api_key=api_key)
    except Exception as e:
        logging.error(f"Failed to initialize Gemini client: {e}")
        gclient = None

    isSnippet = True if x_use_snippet_model == 'true' else False
    systemPrompt = SYSTEM_PROMPT_FOR_SNIPPETS if isSnippet else SYSTEM_PROMPT

    if gclient is None:
        raise HTTPException(
            status_code=503,
            detail="Gemini client is not initialized. Ensure GEMINI_API_KEY is set.",
        )

    user_content = ""

    if not isSnippet:
        user_content = f"\n{request_data.code}\n"
        if request_data.context:
            user_content += f"\nADDITIONAL CONTEXT:\n---\n{request_data.context}\n---"
    else:
        user_content = f"\n{request_data.code}\n"

    async def generate_stream() -> AsyncGenerator[str, None]:
        if gclient is None:
            raise HTTPException(
                status_code=503,
                detail="Gemini client is not initialized. Ensure GEMINI_API_KEY is set.",
            )

        try:
            stream = gclient.models.generate_content_stream(
                model="gemini-2.5-flash",
                contents=[user_content],  
                config=genai.types.GenerateContentConfig(
                    system_instruction=systemPrompt, response_mime_type="text/plain"
                ),
            )

            for chunk in stream:
                if chunk.text:
                    yield chunk.text

        except APIError as e:
            logging.error(f"Gemini API Error: {e}")
            yield f"\n[API_ERROR] Gemini API Error: The service returned an error. Check your API key and quota status. Details: {e}"
        except Exception as e:
            logging.error(f"An unexpected server error occurred: {e}")
            yield f"\n[SERVER_ERROR] An unexpected error occurred: {e}"

    return StreamingResponse(generate_stream(), media_type="text/plain")

@app.post("/analyze_snippet_openai", tags=["Analysis"])
@app.post("/analyze_code_openai", tags=["Analysis"])
async def analyze_code_endpoint_chatgpt(request_data: CodeAnalysisRequest, settings: Annotated[config.Settings, Depends(get_settings)], 
                                        x_use_snippet_model: str | None = Header(default=None), 
                                        x_cloud_api_key: str | None = Header(default=None), 
                                        x_cloud_encrypted_key: str | None = Header(default=None), 
                                        x_cloud_iv: str | None = Header(default=None)):

    api_key = ""
    if x_cloud_api_key and x_cloud_encrypted_key and x_cloud_iv:
        api_key = utils.decrypt_envelope(x_cloud_encrypted_key, x_cloud_iv, x_cloud_api_key, settings.RSA_PRIVATE_KEY)

    client = None
    try:
        # Initialize OpenAI Client
        client = OpenAI(api_key=api_key)
    except Exception as e:
        logging.error(f"Failed to initialize OpenAI client: {e}")
        client = None

    isSnippet = True if x_use_snippet_model == 'true' else False
    systemPrompt = SYSTEM_PROMPT_FOR_SNIPPETS if isSnippet else SYSTEM_PROMPT
    
    # Select appropriate model (e.g., gpt-4o or gpt-4o-mini)
    model_name = "gpt-4o-mini" if isSnippet else "gpt-4o"

    if client is None:
        raise HTTPException(
            status_code=503,
            detail="OpenAI client is not initialized. Ensure API key is valid.",
        )

    user_content = f"\n{request_data.code}\n"
    if not isSnippet and request_data.context:
        user_content += f"\nADDITIONAL CONTEXT:\n---\n{request_data.context}\n---"

    async def generate_stream() -> AsyncGenerator[str, None]:
        try:
            # OpenAI Streaming Logic
            stream = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": systemPrompt},
                    {"role": "user", "content": user_content}
                ],
                stream=True
            )

            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except APIError as e:
            logging.error(f"OpenAI API Error: {e}")
            yield f"\n[API_ERROR] OpenAI API Error: {e}"
        except Exception as e:
            logging.error(f"An unexpected server error occurred: {e}")
            yield f"\n[SERVER_ERROR] An unexpected error occurred: {e}"

    return StreamingResponse(generate_stream(), media_type="text/plain")

@app.post("/analyze_snippet_grok", tags=["Analysis"])
@app.post("/analyze_code_grok", tags=["Analysis"])
async def analyze_code_endpoint_grok(request_data: CodeAnalysisRequest, settings: Annotated[config.Settings, Depends(get_settings)], 
                                     x_use_snippet_model: str | None = Header(default=None), 
                                     x_cloud_api_key: str | None = Header(default=None), 
                                     x_cloud_encrypted_key: str | None = Header(default=None), 
                                     x_cloud_iv: str | None = Header(default=None)):

    api_key = ""
    if x_cloud_api_key and x_cloud_encrypted_key and x_cloud_iv:
        api_key = utils.decrypt_envelope(x_cloud_encrypted_key, x_cloud_iv, x_cloud_api_key, settings.RSA_PRIVATE_KEY)

    client = None
    try:
        # Initialize xAI Client (using OpenAI SDK)
        client = OpenAI(
            api_key=api_key,
            base_url="https://api.x.ai/v1"
        )
    except Exception as e:
        logging.error(f"Failed to initialize Grok client: {e}")
        client = None

    isSnippet = True if x_use_snippet_model == 'true' else False
    systemPrompt = SYSTEM_PROMPT_FOR_SNIPPETS if isSnippet else SYSTEM_PROMPT
    
    # Current Grok beta model
    model_name = "grok-beta" 

    if client is None:
        raise HTTPException(
            status_code=503,
            detail="Grok client is not initialized. Ensure API key is valid.",
        )

    user_content = f"\n{request_data.code}\n"
    if not isSnippet and request_data.context:
        user_content += f"\nADDITIONAL CONTEXT:\n---\n{request_data.context}\n---"

    async def generate_stream() -> AsyncGenerator[str, None]:
        try:
            stream = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": systemPrompt},
                    {"role": "user", "content": user_content}
                ],
                stream=True
            )

            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except APIError as e:
            logging.error(f"Grok API Error: {e}")
            yield f"\n[API_ERROR] Grok API Error: {e}"
        except Exception as e:
            logging.error(f"An unexpected server error occurred: {e}")
            yield f"\n[SERVER_ERROR] An unexpected error occurred: {e}"

    return StreamingResponse(generate_stream(), media_type="text/plain")

@app.post("/analyze_snippet_anthropic", tags=["Analysis"])
@app.post("/analyze_code_anthropic", tags=["Analysis"])
async def analyze_code_endpoint_claude(request_data: CodeAnalysisRequest, settings: Annotated[config.Settings, Depends(get_settings)], 
                                       x_use_snippet_model: str | None = Header(default=None), 
                                       x_cloud_api_key: str | None = Header(default=None), 
                                       x_cloud_encrypted_key: str | None = Header(default=None), 
                                       x_cloud_iv: str | None = Header(default=None)):

    api_key = ""
    if x_cloud_api_key and x_cloud_encrypted_key and x_cloud_iv:
        api_key = utils.decrypt_envelope(x_cloud_encrypted_key, x_cloud_iv, x_cloud_api_key, settings.RSA_PRIVATE_KEY)

    client = None
    try:
        # Initialize Anthropic Client
        client = Anthropic(api_key=api_key)
    except Exception as e:
        logging.error(f"Failed to initialize Claude client: {e}")
        client = None

    isSnippet = True if x_use_snippet_model == 'true' else False
    systemPrompt = SYSTEM_PROMPT_FOR_SNIPPETS if isSnippet else SYSTEM_PROMPT
    
    # Select appropriate model (Haiku for speed/snippets, Sonnet for complex code)
    model_name = "claude-3-haiku-20240307" if isSnippet else "claude-3-5-sonnet-20240620"

    if client is None:
        raise HTTPException(
            status_code=503,
            detail="Claude client is not initialized. Ensure API key is valid.",
        )

    user_content = f"\n{request_data.code}\n"
    if not isSnippet and request_data.context:
        user_content += f"\nADDITIONAL CONTEXT:\n---\n{request_data.context}\n---"

    async def generate_stream() -> AsyncGenerator[str, None]:
        try:
            with client.messages.stream(
                max_tokens=4096,
                system=systemPrompt,
                messages=[
                    {"role": "user", "content": user_content}
                ],
                model=model_name,
            ) as stream:
                for text in stream.text_stream:
                    yield text

        except AnthropicAPIError as e:
            logging.error(f"Claude API Error: {e}")
            yield f"\n[API_ERROR] Claude API Error: {e}"
        except Exception as e:
            logging.error(f"An unexpected server error occurred: {e}")
            yield f"\n[SERVER_ERROR] An unexpected error occurred: {e}"

    return StreamingResponse(generate_stream(), media_type="text/plain")

@app.get("/.well-known/rsa-key", tags=["RSA public key"])
async def get_rsa_public_key():
    return FileResponse(
        path="./rsa_public.pem",
        status_code=200,
        filename="rsa_public.pem"
    )
